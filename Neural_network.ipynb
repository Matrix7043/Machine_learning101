{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZOQ7AYxrvNzo4MlgIQlfH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matrix7043/Machine_learning101/blob/main/Neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oGAtWj6QtJRf"
      },
      "outputs": [],
      "source": [
        "import typing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import ndarray"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(X: ndarray) -> ndarray:\n",
        "    return 1/(1 + np.exp(-1.0 * X))\n"
      ],
      "metadata": {
        "id": "53aHOnVwuZrJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zscore_normalize_features(X: ndarray) -> ndarray:\n",
        "\n",
        "    mu     = np.mean(X, axis=0)\n",
        "    sigma  = np.std(X, axis=0)\n",
        "    X_norm = (X - mu) / sigma\n",
        "\n",
        "    return (X_norm, mu, sigma)"
      ],
      "metadata": {
        "id": "ukRMfyneJWBM"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df0 = pd.read_csv('/content/sample_data/california_housing_train.csv')\n",
        "df1 = df0.drop(columns=['median_house_value'])\n",
        "df2 = df0['median_house_value']\n",
        "x = df1.to_numpy()\n",
        "y = df2.to_numpy().reshape(len(df1), 1)\n",
        "xnorm, xmu, xsig = zscore_normalize_features(x)\n",
        "ynorm, ymu, ysig = zscore_normalize_features(y)\n",
        "print(df0.corr(method='pearson'))"
      ],
      "metadata": {
        "id": "bwsg-9TOGBmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X_batch: ndarray,\n",
        "                        Y_batch: ndarray,\n",
        "                        weights: dict[str, ndarray]) -> tuple[dict[str, ndarray], float]:\n",
        "    #W1 = np.transpose(weights['W1'])\n",
        "    #W2 = np.transpose(weights['W2'])\n",
        "\n",
        "    M1 = np.dot(X_batch, weights['W1'])\n",
        "    # print(\"M1\", M1.shape)\n",
        "\n",
        "    N1 = M1 + weights['B1']\n",
        "    # print(\"N1\", N1.shape)\n",
        "\n",
        "    O1 = sigmoid(N1)\n",
        "    # print(\"O1\", O1.shape)\n",
        "\n",
        "    M2 = np.dot(O1, weights['W2'])\n",
        "    # print(\"M2\", M2.shape)\n",
        "\n",
        "    P = M2 + weights['B2']\n",
        "    # print(\"P\", P.shape)\n",
        "\n",
        "    L = np.mean(np.power((Y_batch - P), 2))\n",
        "\n",
        "    forward_info: dict[str, ndarray] = {}\n",
        "    forward_info['X'] = X_batch\n",
        "    forward_info['Y'] = Y_batch\n",
        "    forward_info['W1'] = weights['W1']\n",
        "    forward_info['W2'] = weights['W2']\n",
        "    forward_info['M1'] = M1\n",
        "    forward_info['M2'] = M2\n",
        "    forward_info['N1'] = N1\n",
        "    forward_info['O1'] = O1\n",
        "    forward_info['P'] = P\n",
        "\n",
        "    return forward_info, L\n"
      ],
      "metadata": {
        "id": "KVb_wmQpt4bk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def back_propogation(forward_info: dict[str, ndarray],\n",
        "                     weights: dict[str, ndarray]) -> dict[str, ndarray]:\n",
        "\n",
        "    dLdP = -1 * (forward_info['Y'] - forward_info['P'])\n",
        "    # print(\"Y\", forward_info['Y'])\n",
        "    # print(\"P\", forward_info['P'])\n",
        "    # print(\"dLdP\", dLdP)\n",
        "\n",
        "    dPdM2 = np.ones_like(forward_info['M2'])\n",
        "    # print(\"dPdM2\", dPdM2.shape)\n",
        "\n",
        "    dM2dO1 = np.transpose(forward_info['W2'])\n",
        "    # print(\"dM2dO1\", dM2dO1.shape)\n",
        "\n",
        "    dO1dN1 = sigmoid(forward_info['N1'] * (1 - sigmoid(forward_info['N1'])))\n",
        "    # print(\"dO1dN1\", dO1dN1.shape)\n",
        "\n",
        "    dN1dM1 = np.ones_like(forward_info['M1'])\n",
        "    # print(\"dN1dM1\", dN1dM1.shape)\n",
        "\n",
        "    dM1dW1 = np.transpose(forward_info['X'])\n",
        "    # print(\"dM1dW1\", dM1dW1.shape)\n",
        "\n",
        "    dN1dB1 = np.ones_like(weights['B1'])\n",
        "    # print(\"dN1dB1\", dN1dB1.shape)\n",
        "\n",
        "    dM2dW2 = np.transpose(forward_info['O1'])\n",
        "    # print(\"dM2dW1\", dM2dW2)\n",
        "\n",
        "    dPdB2 = np.ones_like(weights['B2'])\n",
        "    # print(\"dPdB2\", dPdB2.shape)\n",
        "\n",
        "    dLdM2 = dLdP * dPdM2\n",
        "    # print(\"dLdM2\", dLdM2.shape)\n",
        "\n",
        "    dLdO1 = np.dot(dLdM2, dM2dO1)\n",
        "    # print(\"dLdO1\", dLdO1.shape)\n",
        "\n",
        "    dLdM1 = (dLdO1 * dO1dN1) * dN1dM1\n",
        "    # print(\"dLdM1\", dLdM1.shape)\n",
        "\n",
        "    # Important\n",
        "\n",
        "    dLdW1 = np.dot(dM1dW1, dLdM1)\n",
        "\n",
        "    dLdW2 = np.dot(dM2dW2, dLdP)\n",
        "\n",
        "    dLdB1 = ((dLdO1 * dO1dN1) * dN1dB1).sum(axis=0)\n",
        "\n",
        "    dLdB2 = (dLdP * dPdB2).sum(axis=0)\n",
        "\n",
        "    loss_gradient: dict[str, ndarray] = {}\n",
        "    loss_gradient['W1'] = dLdW1\n",
        "    loss_gradient['W2'] = dLdW2\n",
        "    loss_gradient['B1'] = dLdB1\n",
        "    loss_gradient['B2'] = dLdB2\n",
        "\n",
        "    return loss_gradient\n"
      ],
      "metadata": {
        "id": "5oUaY7s3zT6D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X: ndarray,\n",
        "            weights: dict[str, ndarray]) -> ndarray:\n",
        "\n",
        "    M1 = np.dot(X, weights['W1'])\n",
        "    N1 = M1 + weights['B1']\n",
        "    O1 = sigmoid(N1)\n",
        "    M2 = np.dot(O1, weights['W2'])\n",
        "    P = M2 + weights['B2']\n",
        "\n",
        "    return P"
      ],
      "metadata": {
        "id": "8kk3FBUx54Sc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(X_test: ndarray,\n",
        "         Y_test: ndarray,\n",
        "         weights: dict[str, ndarray]) -> tuple[float, float]:\n",
        "\n",
        "    def mae(Y: ndarray,\n",
        "            P: ndarray) -> float:\n",
        "\n",
        "        return np.mean(Y - P)\n",
        "\n",
        "    def rmse(Y: ndarray,\n",
        "             P: ndarray) -> float:\n",
        "\n",
        "        return np.power(np.mean(np.power(P - Y, 2)), 1/2)\n",
        "\n",
        "    P = predict(X_test, weights)\n",
        "\n",
        "    return (rmse(Y_test, P), mae(Y_test, P))"
      ],
      "metadata": {
        "id": "9iz8wa8p6yDb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_batch: ndarray,\n",
        "          Y_batch: ndarray,\n",
        "          learning_rate: float = 0.00000001,\n",
        "          hidden_size: int = 13,\n",
        "          iterations: int = 5000,\n",
        "          split: float = 0.7) -> dict[str, ndarray]:\n",
        "\n",
        "    total = X_batch.shape[0]\n",
        "    num = int(total*split)\n",
        "\n",
        "    X_train: ndarray = X_batch[:num:]\n",
        "    Y_train: ndarray = Y_batch[:num:]\n",
        "    # print(X_train.shape)\n",
        "    # print(Y_train.shape)\n",
        "\n",
        "    X_test: ndarray = X_batch[num::]\n",
        "    Y_test: ndarray = Y_batch[num::]\n",
        "    # print(X_test.shape)\n",
        "    # print(Y_test.shape)\n",
        "\n",
        "    def init_weights(input_size: int,\n",
        "                     hidden_size: int) -> dict[str, ndarray]:\n",
        "\n",
        "              weights: dict[str, ndarray] = {}\n",
        "              weights['W1'] = np.random.randn(input_size, hidden_size)\n",
        "              weights['B1'] = np.random.randn(1, hidden_size)\n",
        "              weights['W2'] = np.random.randn(hidden_size, 1)\n",
        "              weights['B2'] = np.random.randn(1)\n",
        "\n",
        "              #print(\"W1\", weights['W1'].shape)\n",
        "              #print(\"B1\", weights['B1'].shape)\n",
        "              #print(\"W2\", weights['W2'].shape)\n",
        "              #print(\"B2\", weights['B2'].shape)\n",
        "              return weights\n",
        "\n",
        "    weights = init_weights(X_train.shape[1], hidden_size)\n",
        "\n",
        "    for i in range(iterations):\n",
        "\n",
        "        forward_info, loss = forward_propagation(X_train, Y_train, weights)\n",
        "        loss_grads = back_propogation(forward_info, weights)\n",
        "\n",
        "        if i%10 == 0:\n",
        "            print(loss)\n",
        "\n",
        "        for key in weights.keys():\n",
        "            weights[key] -= learning_rate * loss_grads[key]\n",
        "\n",
        "    theta = test(X_test, Y_test, weights)\n",
        "    print(theta)\n"
      ],
      "metadata": {
        "id": "hpnMQkAN88V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(xnorm, ynorm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ohsYwKqA3HS",
        "outputId": "b840f5a3-b1b3-4ef6-849c-af8f306cdcef"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11900, 8)\n",
            "(11900, 1)\n",
            "(5100, 8)\n",
            "(5100, 1)\n",
            "2.0307490732557203\n",
            "2.0191743402198172\n",
            "2.007719307782981\n",
            "1.9963826044347255\n",
            "1.9851628749538983\n",
            "1.9740587802083591\n",
            "1.963068996957258\n",
            "1.9521922176558835\n",
            "1.9414271502630414\n",
            "1.9307725180509319\n",
            "1.9202270594174864\n",
            "1.909789527701128\n",
            "1.8994586909979283\n",
            "1.8892333319811183\n",
            "1.8791122477229312\n",
            "1.8690942495187286\n",
            "1.8591781627133954\n",
            "1.84936282652996\n",
            "1.8396470939004144\n",
            "1.8300298312986996\n",
            "1.8205099185758291\n",
            "1.8110862487971235\n",
            "1.8017577280815205\n",
            "1.7925232754429343\n",
            "1.7833818226336386\n",
            "1.7743323139896443\n",
            "1.7653737062780395\n",
            "1.7565049685462752\n",
            "1.7477250819733576\n",
            "1.7390330397229306\n",
            "1.7304278467982175\n",
            "1.7219085198988033\n",
            "1.7134740872792182\n",
            "1.705123588609318\n",
            "1.6968560748364163\n",
            "1.688670608049161\n",
            "1.6805662613431227\n",
            "1.6725421186880765\n",
            "1.6645972747969477\n",
            "1.6567308349964078\n",
            "1.6489419150990907\n",
            "1.6412296412774148\n",
            "1.6335931499389817\n",
            "1.626031587603538\n",
            "1.6185441107814695\n",
            "1.6111298858538208\n",
            "1.6037880889538076\n",
            "1.5965179058498087\n",
            "1.589318531829812\n",
            "1.5821891715873069\n",
            "1.5751290391085877\n",
            "1.568137357561466\n",
            "1.5612133591853574\n",
            "1.5543562851827388\n",
            "1.5475653856119467\n",
            "1.540839919281306\n",
            "1.534179153644573\n",
            "1.5275823646976636\n",
            "1.5210488368766666\n",
            "1.5145778629571107\n",
            "1.5081687439544793\n",
            "1.5018207890259503\n",
            "1.4955333153733463\n",
            "1.4893056481472788\n",
            "1.483137120352482\n",
            "1.4770270727542925\n",
            "1.4709748537862959\n",
            "1.4649798194590982\n",
            "1.4590413332702217\n",
            "1.4531587661151024\n",
            "1.447331496199178\n",
            "1.4415589089510543\n",
            "1.435840396936738\n",
            "1.4301753597749098\n",
            "1.4245632040532419\n",
            "1.419003343245732\n",
            "1.4134951976310495\n",
            "1.4080381942118785\n",
            "1.4026317666352381\n",
            "1.3972753551137842\n",
            "1.391968406348059\n",
            "1.3867103734496942\n",
            "1.381500715865545\n",
            "1.376338899302748\n",
            "1.3712243956546855\n",
            "1.3661566829278546\n",
            "1.3611352451696204\n",
            "1.356159572396845\n",
            "1.3512291605253823\n",
            "1.3463435113004296\n",
            "1.3415021322277163\n",
            "1.3367045365055317\n",
            "1.3319502429575723\n",
            "1.3272387759665962\n",
            "1.3225696654088896\n",
            "1.3179424465895155\n",
            "1.3133566601783475\n",
            "1.3088118521468741\n",
            "1.3043075737057663\n",
            "1.2998433812431958\n",
            "1.2954188362638972\n",
            "1.2910335053289659\n",
            "1.2866869599963744\n",
            "1.2823787767622097\n",
            "1.2781085370026164\n",
            "1.273875826916434\n",
            "1.2696802374685274\n",
            "1.2655213643337933\n",
            "1.261398807841841\n",
            "1.2573121729223384\n",
            "1.25326106905101\n",
            "1.2492451101962823\n",
            "1.2452639147665698\n",
            "1.2413171055581929\n",
            "1.237404309703914\n",
            "1.2335251586220979\n",
            "1.2296792879664702\n",
            "1.2258663375764842\n",
            "1.2220859514282751\n",
            "1.2183377775862048\n",
            "1.2146214681549827\n",
            "1.2109366792323542\n",
            "1.2072830708623623\n",
            "1.2036603069891585\n",
            "1.2000680554113672\n",
            "1.1965059877369943\n",
            "1.1929737793388728\n",
            "1.1894711093106376\n",
            "1.1859976604232254\n",
            "1.1825531190818908\n",
            "1.1791371752837339\n",
            "1.175749522575736\n",
            "1.1723898580132894\n",
            "1.169057882119223\n",
            "1.1657532988433117\n",
            "1.162475815522268\n",
            "1.1592251428402125\n",
            "1.1560009947896037\n",
            "1.1528030886326395\n",
            "1.1496311448631145\n",
            "1.1464848871687279\n",
            "1.1433640423938376\n",
            "1.1402683405026577\n",
            "1.137197514542889\n",
            "1.1341513006097834\n",
            "1.1311294378106311\n",
            "1.1281316682296667\n",
            "1.125157736893395\n",
            "1.1222073917363213\n",
            "1.119280383567091\n",
            "1.1163764660350273\n",
            "1.1134953955970632\n",
            "1.1106369314850677\n",
            "1.107800835673552\n",
            "1.1049868728477632\n",
            "1.1021948103721497\n",
            "1.0994244182592012\n",
            "1.096675469138655\n",
            "1.0939477382270653\n",
            "1.0912410032977313\n",
            "1.0885550446509815\n",
            "1.0858896450848046\n",
            "1.0832445898658292\n",
            "1.0806196667006431\n",
            "1.0780146657074532\n",
            "1.0754293793880738\n",
            "1.0728636026002523\n",
            "1.0703171325303114\n",
            "1.0677897686661206\n",
            "1.065281312770379\n",
            "1.0627915688542204\n",
            "1.0603203431511192\n",
            "1.057867444091109\n",
            "1.055432682275303\n",
            "1.0530158704507138\n",
            "1.0506168234853652\n",
            "1.048235358343703\n",
            "1.045871294062287\n",
            "1.043524451725774\n",
            "1.0411946544431805\n",
            "1.0388817273244213\n",
            "1.0365854974571287\n",
            "1.0343057938837372\n",
            "1.032042447578843\n",
            "1.0297952914268238\n",
            "1.0275641601997236\n",
            "1.0253488905353956\n",
            "1.0231493209159015\n",
            "1.020965291646165\n",
            "1.018796644832869\n",
            "1.0166432243636112\n",
            "1.0145048758862933\n",
            "1.0123814467887564\n",
            "1.0102727861786525\n",
            "1.008178744863554\n",
            "1.0060991753312918\n",
            "1.0040339317305291\n",
            "1.0019828698515576\n",
            "0.9999458471073179\n",
            "0.9979227225146472\n",
            "0.9959133566757382\n",
            "0.9939176117598226\n",
            "0.9919353514850628\n",
            "0.9899664411006572\n",
            "0.9880107473691547\n",
            "0.9860681385489758\n",
            "0.9841384843771365\n",
            "0.9822216560521762\n",
            "0.980317526217282\n",
            "0.9784259689436113\n",
            "0.9765468597138103\n",
            "0.9746800754057223\n",
            "0.9728254942762877\n",
            "0.970982995945631\n",
            "0.9691524613813334\n",
            "0.9673337728828901\n",
            "0.9655268140663464\n",
            "0.9637314698491147\n",
            "0.9619476264349682\n",
            "0.960175171299209\n",
            "0.9584139931740083\n",
            "0.95666398203392\n",
            "0.9549250290815592\n",
            "0.9531970267334514\n",
            "0.9514798686060443\n",
            "0.9497734495018819\n",
            "0.9480776653959414\n",
            "0.9463924134221287\n",
            "0.9447175918599284\n",
            "0.9430531001212145\n",
            "0.9413988387372094\n",
            "0.9397547093455973\n",
            "0.9381206146777872\n",
            "0.9364964585463235\n",
            "0.9348821458324427\n",
            "0.9332775824737767\n",
            "0.9316826754521976\n",
            "0.9300973327818028\n",
            "0.9285214634970429\n",
            "0.9269549776409836\n",
            "0.9253977862537086\n",
            "0.9238498013608529\n",
            "0.9223109359622718\n",
            "0.9207811040208418\n",
            "0.9192602204513896\n",
            "0.9177482011097524\n",
            "0.916244962781963\n",
            "0.9147504231735616\n",
            "0.9132645008990311\n",
            "0.9117871154713558\n",
            "0.9103181872917018\n",
            "0.9088576376392147\n",
            "0.9074053886609386\n",
            "0.9059613633618515\n",
            "0.9045254855950143\n",
            "0.9030976800518373\n",
            "0.9016778722524558\n",
            "0.9002659885362219\n",
            "0.8988619560523035\n",
            "0.8974657027503928\n",
            "0.8960771573715225\n",
            "0.8946962494389908\n",
            "0.8933229092493877\n",
            "0.8919570678637285\n",
            "0.8905986570986899\n",
            "0.8892476095179449\n",
            "0.8879038584236019\n",
            "0.8865673378477408\n",
            "0.8852379825440477\n",
            "0.8839157279795463\n",
            "0.8826005103264263\n",
            "0.8812922664539646\n",
            "0.879990933920542\n",
            "0.8786964509657511\n",
            "0.8774087565025965\n",
            "0.8761277901097847\n",
            "0.8748534920241031\n",
            "0.8735858031328892\n",
            "0.8723246649665835\n",
            "0.8710700196913738\n",
            "0.8698218101019173\n",
            "0.8685799796141558\n",
            "0.8673444722582071\n",
            "0.8661152326713413\n",
            "0.8648922060910391\n",
            "0.8636753383481293\n",
            "0.8624645758600061\n",
            "0.8612598656239245\n",
            "0.8600611552103754\n",
            "0.8588683927565331\n",
            "0.857681526959784\n",
            "0.8565005070713254\n",
            "0.8553252828898413\n",
            "0.8541558047552503\n",
            "0.8529920235425252\n",
            "0.8518338906555865\n",
            "0.8506813580212637\n",
            "0.8495343780833275\n",
            "0.8483929037965924\n",
            "0.8472568886210855\n",
            "0.8461262865162827\n",
            "0.8450010519354141\n",
            "0.8438811398198326\n",
            "0.8427665055934481\n",
            "0.8416571051572275\n",
            "0.840552894883756\n",
            "0.8394538316118647\n",
            "0.8383598726413152\n",
            "0.8372709757275502\n",
            "0.8361870990765039\n",
            "0.8351082013394698\n",
            "0.8340342416080314\n",
            "0.8329651794090474\n",
            "0.8319009746997\n",
            "0.8308415878625953\n",
            "0.8297869797009232\n",
            "0.8287371114336728\n",
            "0.8276919446909029\n",
            "0.826651441509067\n",
            "0.8256155643263913\n",
            "0.8245842759783091\n",
            "0.8235575396929443\n",
            "0.8225353190866491\n",
            "0.8215175781595939\n",
            "0.8205042812914056\n",
            "0.8194953932368582\n",
            "0.8184908791216131\n",
            "0.8174907044380081\n",
            "0.8164948350408948\n",
            "0.8155032371435239\n",
            "0.8145158773134785\n",
            "0.8135327224686554\n",
            "0.8125537398732907\n",
            "0.8115788971340319\n",
            "0.8106081621960558\n",
            "0.8096415033392317\n",
            "0.8086788891743268\n",
            "0.8077202886392586\n",
            "0.8067656709953873\n",
            "0.8058150058238513\n",
            "0.8048682630219496\n",
            "0.8039254127995586\n",
            "0.8029864256755964\n",
            "0.8020512724745233\n",
            "0.8011199243228874\n",
            "0.8001923526459054\n",
            "0.7992685291640856\n",
            "0.798348425889889\n",
            "0.7974320151244295\n",
            "0.7965192694542107\n",
            "0.7956101617479018\n",
            "0.7947046651531502\n",
            "0.7938027530934298\n",
            "0.792904399264929\n",
            "0.7920095776334692\n",
            "0.7911182624314638\n",
            "0.7902304281549093\n",
            "0.7893460495604117\n",
            "0.7884651016622487\n",
            "0.7875875597294637\n",
            "0.7867133992829936\n",
            "0.7858425960928306\n",
            "0.784975126175217\n",
            "0.7841109657898698\n",
            "0.7832500914372413\n",
            "0.7823924798558068\n",
            "0.7815381080193877\n",
            "0.7806869531345005\n",
            "0.7798389926377433\n",
            "0.778994204193205\n",
            "0.7781525656899095\n",
            "0.7773140552392869\n",
            "0.7764786511726741\n",
            "0.7756463320388445\n",
            "0.7748170766015676\n",
            "0.7739908638371934\n",
            "0.7731676729322676\n",
            "0.7723474832811713\n",
            "0.7715302744837922\n",
            "0.7707160263432183\n",
            "0.7699047188634603\n",
            "0.7690963322471993\n",
            "0.7682908468935618\n",
            "0.7674882433959187\n",
            "0.76668850253971\n",
            "0.7658916053002962\n",
            "0.7650975328408315\n",
            "0.7643062665101628\n",
            "0.7635177878407552\n",
            "0.7627320785466359\n",
            "0.7619491205213689\n",
            "0.7611688958360457\n",
            "0.7603913867373048\n",
            "0.7596165756453693\n",
            "0.7588444451521119\n",
            "0.7580749780191385\n",
            "0.7573081571758954\n",
            "0.756543965717797\n",
            "0.7557823869043768\n",
            "0.7550234041574587\n",
            "0.7542670010593486\n",
            "0.753513161351048\n",
            "0.7527618689304871\n",
            "0.7520131078507795\n",
            "0.7512668623184959\n",
            "0.7505231166919576\n",
            "0.74978185547955\n",
            "0.7490430633380553\n",
            "0.7483067250710048\n",
            "0.7475728256270493\n",
            "0.7468413500983486\n",
            "0.74611228371898\n",
            "0.7453856118633634\n",
            "0.7446613200447078\n",
            "0.7439393939134709\n",
            "0.7432198192558405\n",
            "0.7425025819922317\n",
            "0.7417876681758014\n",
            "0.7410750639909798\n",
            "0.7403647557520179\n",
            "0.7396567299015541\n",
            "0.7389509730091951\n",
            "0.7382474717701129\n",
            "0.7375462130036601\n",
            "0.7368471836519985\n",
            "0.7361503707787455\n",
            "0.7354557615676343\n",
            "0.7347633433211904\n",
            "0.7340731034594243\n",
            "0.7333850295185361\n",
            "0.7326991091496385\n",
            "0.7320153301174916\n",
            "0.7313336802992547\n",
            "0.730654147683249\n",
            "0.7299767203677394\n",
            "0.7293013865597251\n",
            "0.7286281345737478\n",
            "0.7279569528307107\n",
            "0.7272878298567146\n",
            "0.7266207542819029\n",
            "0.7259557148393232\n",
            "0.725292700363801\n",
            "0.724631699790826\n",
            "0.723972702155451\n",
            "0.7233156965912035\n",
            "0.722660672329011\n",
            "0.7220076186961366\n",
            "0.7213565251151276\n",
            "0.7207073811027768\n",
            "0.7200601762690952\n",
            "0.7194149003162956\n",
            "0.7187715430377895\n",
            "0.7181300943171935\n",
            "0.7174905441273505\n",
            "0.7168528825293555\n",
            "0.7162170996716015\n",
            "0.7155831857888278\n",
            "0.7149511312011857\n",
            "0.7143209263133097\n",
            "0.7136925616134038\n",
            "0.713066027672335\n",
            "0.7124413151427385\n",
            "0.7118184147581333\n",
            "0.7111973173320469\n",
            "0.7105780137571515\n",
            "0.7099604950044085\n",
            "0.7093447521222238\n",
            "0.7087307762356122\n",
            "0.7081185585453721\n",
            "0.7075080903272677\n",
            "0.7068993629312239\n",
            "0.7062923677805268\n",
            "0.7056870963710354\n",
            "0.7050835402704021\n",
            "0.7044816911173012\n",
            "0.7038815406206673\n",
            "0.7032830805589418\n",
            "0.7026863027793278\n",
            "0.702091199197054\n",
            "0.7014977617946462\n",
            "0.700905982621209\n",
            "0.7003158537917122\n",
            "0.6997273674862898\n",
            "0.6991405159495427\n",
            "0.6985552914898528\n",
            "0.6979716864787018\n",
            "0.6973896933500014\n",
            "0.6968093045994269\n",
            "0.6962305127837622\n",
            "0.6956533105202499\n",
            "0.6950776904859497\n",
            "0.6945036454171035\n",
            "0.6939311681085086\n",
            "0.693360251412898\n",
            "0.6927908882403261\n",
            "0.6922230715575634\n",
            "0.6916567943874973\n",
            "0.6910920498085393\n",
            "0.69052883095404\n",
            "0.6899671310117094\n",
            "(0.8960857743669026, -0.11086620163889575)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WoJtkuAhKpcQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}